{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ananconda3\\conda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#for os and paths\n",
    "import os\n",
    "import sys\n",
    "# mathematical \n",
    "import numpy as np\n",
    "# datahandling \n",
    "import pandas as pd\n",
    "# to plot graphs and visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# natural lanuage processing \n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for gpu training\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "keras for deep learning models\n",
    "\n",
    "Preprocessing Imports\n",
    "'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "'''\n",
    "Different Neural Network Layers\n",
    "'''\n",
    "from keras.layers import Dense,  Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPool1D, Embedding\n",
    "from keras.layers import Dropout \n",
    "'''\n",
    "Build Model\n",
    "'''\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set configurations and dimensions \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to data \n",
    "train_data_path = './toxic_comments_dataset/train.csv'\n",
    "test_data_path = './toxic_comments_dataset/test.csv'\n",
    "\n",
    "# path to GolVe\n",
    "glove_path = './glove.6B/glove.6B.{0}d.txt'.format(EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec...\n",
      "number of vectors : 400000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loading word2vectors from GloVe\n",
    "'''\n",
    "print ('loading word2vec...')\n",
    "\n",
    "word2vec = {}\n",
    "\n",
    "with open(os.path.join(glove_path), encoding='utf8') as fs:\n",
    "    for line in fs:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print ('number of vectors : {0}'.format(len(word2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "loading training data\n",
    "'''\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading all row wise comment_text data into sentences\n",
    "sentences = train_data['comment_text'].fillna('DUMMY_VALUES').values\n",
    "# storing labels intp possible_labels\n",
    "possible_labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "# loading all row wise possible_labels into target\n",
    "targets = train_data[possible_labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    converting sentences into interger sequences\n",
    "\n",
    "'''\n",
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
    "# downsizing or fitting the sentences into respective tokens\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# transforming text to integer sequences \n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type <class 'list'>\n",
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 12052, 1116, 86, 331, 51, 2278, 11448, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 15190, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n"
     ]
    }
   ],
   "source": [
    "print('sequences type', type(sequences))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum sequence length : 1400\n",
      "minimum sequence length : 0\n",
      "median sequences length : 35\n"
     ]
    }
   ],
   "source": [
    "len_seq = [len(each_seq) for each_seq in sequences]\n",
    "print('maximum sequence length : {0}'.format(max(len_seq)))\n",
    "print('minimum sequence length : {0}'.format(min(len_seq)))\n",
    "len_seq = sorted(len_seq)\n",
    "idx = len(len_seq)//2\n",
    "print ('median sequences length : {0}'.format(len_seq[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210337\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# map word to integer [indexing]\n",
    "word_index = tokenizer.word_index\n",
    "# number of unique words\n",
    "print(len(word_index))\n",
    "# type \n",
    "print(type(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data (159571, 200)\n"
     ]
    }
   ],
   "source": [
    "# convert all different input sizes into constant size of max_sequence_length\n",
    "data = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "# checking shape of data\n",
    "print('shape of data {0}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n",
      "shape of embedding matrix is (20000, 100)\n"
     ]
    }
   ],
   "source": [
    "# preparing embedding matrix \n",
    "print('Filling pre-trained embeddings...')\n",
    "\n",
    "num_words = min(MAX_VOCAB_SIZE,len(word_index)+1)\n",
    "\n",
    "# initially populate embedding matrix to be all zeros\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # words which are found will be updated\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#shape of embedding_matrix\n",
    "print('shape of embedding matrix is {0}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a embeddings object for neural net using pretrained weights\n",
    "embedding_layer = Embedding(\n",
    "num_words,\n",
    "EMBEDDING_DIM,\n",
    "weights =[embedding_matrix],\n",
    "input_length = MAX_SEQUENCE_LENGTH,\n",
    "trainable = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the Model...\n"
     ]
    }
   ],
   "source": [
    "print('Building the Model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "x = embedding_layer(input_)\n",
    "\n",
    "x = Conv1D(128,3,activation = 'relu')(x)\n",
    "\n",
    "x = MaxPool1D(3)(x)\n",
    "\n",
    "x = Conv1D(128,3, activation = 'relu')(x)\n",
    "\n",
    "x = MaxPool1D(3)(x)\n",
    "\n",
    "x = Conv1D(128,3,activation = 'relu')(x)\n",
    "\n",
    "x = Dropout(0.3)(x) \n",
    "\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = Dense(128, activation ='relu')(x)\n",
    "\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output = Dense(len(possible_labels), activation = 'sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input_, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile( \n",
    "loss = 'binary_crossentropy',\n",
    "optimizer = 'rmsprop',\n",
    "metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/10\n",
      "127656/127656 [==============================] - 419s 3ms/step - loss: 0.5947 - acc: 0.9616 - val_loss: 0.5271 - val_acc: 0.9623\n",
      "Epoch 2/10\n",
      "127656/127656 [==============================] - 399s 3ms/step - loss: 0.3538 - acc: 0.9628 - val_loss: 0.1508 - val_acc: 0.9635\n",
      "Epoch 3/10\n",
      "127656/127656 [==============================] - 423s 3ms/step - loss: 0.1403 - acc: 0.9633 - val_loss: 0.1579 - val_acc: 0.9635\n",
      "Epoch 4/10\n",
      "127656/127656 [==============================] - 448s 4ms/step - loss: 0.1395 - acc: 0.9633 - val_loss: 0.1477 - val_acc: 0.9635\n",
      "Epoch 5/10\n",
      "127656/127656 [==============================] - 460s 4ms/step - loss: 0.1393 - acc: 0.9633 - val_loss: 0.1387 - val_acc: 0.9635\n",
      "Epoch 6/10\n",
      "127656/127656 [==============================] - 448s 4ms/step - loss: 0.1390 - acc: 0.9633 - val_loss: 0.1425 - val_acc: 0.9635\n",
      "Epoch 7/10\n",
      "127656/127656 [==============================] - 438s 3ms/step - loss: 0.1389 - acc: 0.9633 - val_loss: 0.1397 - val_acc: 0.9635\n",
      "Epoch 8/10\n",
      "127656/127656 [==============================] - 370s 3ms/step - loss: 0.1389 - acc: 0.9633 - val_loss: 0.1378 - val_acc: 0.9635\n",
      "Epoch 9/10\n",
      "127656/127656 [==============================] - 469s 4ms/step - loss: 0.1387 - acc: 0.9633 - val_loss: 0.1378 - val_acc: 0.9635\n",
      "Epoch 10/10\n",
      "127656/127656 [==============================] - 383s 3ms/step - loss: 0.1387 - acc: 0.9633 - val_loss: 0.1378 - val_acc: 0.9635\n"
     ]
    }
   ],
   "source": [
    "print('Training Model...')\n",
    "r = model.fit(\n",
    "    data,\n",
    "    targets,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_split = VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing on test data\n",
    "test_data_sentences = test_data['comment_text'].fillna('DUMMY_VALUES').values\n",
    "test_data_sequences = tokenizer.texts_to_sequences(test_data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_feed =  pad_sequences(test_data_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = model.predict(test_data_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = './toxic_comments_dataset/sample_submission.csv'\n",
    "\n",
    "submission  = pd.read_csv(submission_path)\n",
    "\n",
    "submission[possible_labels] = predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
